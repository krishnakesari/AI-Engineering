{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Regression\n",
    "- Can we predict one variable based on other variables\n",
    "- Predict continous value\n",
    "- Dependent variable (Final goal or Y), Independent variable (target variables or X)\n",
    "\n",
    "## Applications\n",
    "- Sales forecasting\n",
    "- Satisfaction Analysis \n",
    "- Price estimation\n",
    "- Employment Income\n",
    "\n",
    "## Simple Regression\n",
    "  Using one variable to predict a value of other variable\n",
    "1. Simple Linear Regression\n",
    "2. Simple Non-linar Regression\n",
    "\n",
    "## Multiple Regression\n",
    "  Using more than one varaibles to predict a value of other variable\n",
    "1. Multiple Linear Regression\n",
    "2. Multiple Non-Linear Regression "
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Simple Linear Regression\n",
    "- Predict Co2 emissions (Continuous value) using engine size, cylinders, fuel consumption\n",
    "- Dependent variable must have continuous values & Independent variables can have continuous/discreet/categorical values\n",
    "\n",
    "## Changes in one variable should explain other variable\n",
    "-  $\\hat{y}$ = $\\theta_0 + \\theta_1 x_1$\n",
    "1. $\\hat{y}$ is response variable\n",
    "2. $x_1$ is a single predictor\n",
    "3. $\\theta_0$ is an intercept\n",
    "4. $\\theta_1$ is slope\n",
    "5. $\\theta_0$ and $\\theta_1$ are also called coefficients of linear equation\n",
    "\n",
    "## Finding line of best fit:\n",
    "- Most important part of Linear regression is to find theta0 and theta 1 \n",
    "- How we can adjust parameters to best fit theta ? \n",
    "### How to find best fit ?\n",
    "1. $x_1$ = 5.4 ; independent variable y = 250\n",
    "2. $\\hat{y}$ = 340 (i.e. after applying $\\hat{y}$ = $\\theta_0$ + $\\theta_1$ * $x_1$)\n",
    "3. Find Residual Error:\n",
    "   - *Distance from data point to fitted regression line*\n",
    "   - Error = $y - \\hat{y}$ = 250 - 340 = -90\n",
    "4. Find Mean Squared Error:\n",
    "   - *Mean of all Residual Errors shows how poorly the line fits with the data set*\n",
    "   - MSE = $\\frac{1}{n} \\Sigma_{i=1}^n({y}-\\hat{y})^2$\n",
    "\n",
    "#### Note: Objective of Linear Regression is to minimize the MSE equation \n",
    "\n",
    "### Mathematical Approach: \n",
    "1. calculate $\\bar{x}$ (Avearge value of independent variable)\n",
    "2. calculate $\\bar{y}$ (Average value of dependent variable)\n",
    "3. Plug $\\bar{x}$ bar and $\\bar{y}$ values in slope equation to find theta1\n",
    "4. Find intercept $\\theta_0$ based on slope $\\theta_1, y, \\hat{y}$\n",
    "#### Note: Theta0 is also called bias coefficient and Theta1 is coefficient for independent variable column\n",
    "\n",
    "### Advantage of Linear Regression\n",
    "   - Very Fast\n",
    "   - No Parameter Tuning Needed\n",
    "   - Easy to understand and highly interpretable "
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Model Evaluation in Regression Models\n",
    "## Accuray of Model\n",
    "### Train and test on same dataset\n",
    "- Train set uses entire dataset and build a model\n",
    "- Test set includes a selected small portion of dataset without labels. (Note: Train set has labels but we should not use them )\n",
    "- Labels are used only for ground truthing\n",
    "- Labels are called \"Actual values of Test Set\" and the values predicted by Test set is called \"Predicted Values\"\n",
    "\n",
    "#### Measuring model accuracy\n",
    "- Difference between actual and predicted values\n",
    "- Error = $\\frac{1}{n} \\Sigma_{j=1}^n |y_j - \\hat{y}_j| $\n",
    "\n",
    "##### Common Pitfalls to Note:\n",
    "- **High \"Training accuracy\"**\n",
    "  - Hight Training accuracy is not necessarily a good thing \n",
    "  - May result in over fitting \n",
    "    - Over fit: The model is overly trained to the dataset, which may capture noise and produce a non-generalized model\n",
    "- **Low \"out-of-sample accurary\"**\n",
    "  - It is important that our models have a high, out of sample accuracy (beacuse our goal is to predict values with high accuracy)\n",
    "  - How can we improve out-of-sample accuracy ? **use Train/Test Split**\n",
    "\n",
    "### Train/Test Split\n",
    "- Train set uses portion of dataset and build model\n",
    "- Test set uses other portion of dataset passed on to the model for prediction\n",
    "- Compared predicted values (from train set) with actual values (from test set)\n",
    "- **Mutually Exclusive**\n",
    "\n",
    "##### Note:\n",
    "- More accurate evalution on out-of-sample accuracy\n",
    "- Train your model with testing set afterwards as you don't want to loose potentially valuable data\n",
    "- Outcome of Train/Test Split is highly dependent on which dataset the data is trained and tested\n",
    "\n",
    "### K-fold cross-validation\n",
    "- If k = 4 fold, we split up data set into \n",
    "- 1st fold - we use first 25% of dataset for testing and rest for training (Model build based on training set and evaluated using test set - accuracy is calculated)\n",
    "- 2nd fold - Second 25% data set is used for testing and rest for training (accurarcy calculated)\n",
    "- 3rd fold - third 25 %\n",
    "- 4th fold - fourth 25% \n",
    "\n",
    "#### Accuracy = average of all four model accuracies"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Evaluation Metrics in Regression Models\n",
    "- Used to explain performance of a model\n",
    "1. Mean Absolute Error (MAE) \n",
    "   - Mean of the absolute value of the error\n",
    "2. Mean Squared Error (MSE)\n",
    "   - Mean of the squared error\n",
    "   - Popular beacuse it focuses on large errors\n",
    "3. Root Mean Squared Error (RMSE)\n",
    "   - Sqare root of the mean squared error \n",
    "4. Relative Absolute Error (RAE)\n",
    "   - takes y bar value and normalizes error \n",
    "5. Relative Squared Error (RSE)\n",
    "   - Used for R-Squared value;  $R^2$ = (1 - RSE);\n",
    "   - $R^2$ is a popular metric for measuring accuracy of the model\n",
    "   - $R^2$ shows how close the data values to the fitted regression line\n",
    "   - Higher $R^2$ means the better model fits the data\n",
    "  "
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Multiple Linear Regression\n",
    "- Predicting Co2 emission based on engine and cylinder of all cars\n",
    "### Usage: \n",
    "1. To identify strength of the effect of independent variable have on dependent variable \n",
    "   - Eg: Does lecutre attendance and gender have any effect on exam performance of students ? \n",
    "2. To predict impacts of changes\n",
    "   - To identify how dependent variable changes when we change independent variables\n",
    "   - Eg: How a patient blood presure increase/decrease for every unit increase/ decrease in BMI (holding other factors constant)\n",
    "\n",
    "#### Note: \n",
    "1. In Multiple linear regression Independent variable (Y) is a linear combination of dependent variables (X)\n",
    "    - $\\hat{y} = \\theta_0 + \\theta_1 * Engine size + \\theta_2 * Cylinders+ ....$\n",
    "2. You can identify \n",
    "   - which variable are significant to outcome variable\n",
    "   - how each feature impacts the outcome variable\n",
    "3. Predict unknown value \n",
    "\n",
    "### Mathematical Notation:\n",
    "- $\\hat{y} = \\theta_0 + \\theta_1x_1 + \\theta_2x_2+ ....+ \\theta_nx_n$\n",
    "- $\\hat{y} = \\theta^TX$(.i.e. Theta Transposed X)\n",
    "  - n X 1 vector of unknown parameters in multidimensional space\n",
    "  - X is the vector of feature sets \n",
    "    - First value of the feature set is set to 1 to accommodate bias parameter theta0\n",
    "  - theta is the vector of coefficients (also called parameters / weight of regression equation)\n",
    "\n",
    "### Important Note:\n",
    "- Due to multiple dependent variable, we don't have a line anymore so we call it Plane / Hyperplane\n",
    "- Our goal is to find best fit hyper plane for data\n",
    "\n",
    "### How to find optimized parameters: \n",
    "- Optimized parameters would decrease the model error in hyperplance\n",
    "##### How it works ?\n",
    "- $y$ (actual) = 196\n",
    "- $\\hat{y}$ (predicted) = 140\n",
    "- residual error = $y - \\hat{y}$ = 196 - 140 = 56\n",
    "\n",
    "### Errors:\n",
    "- Mean Squared Error (MSE): most popular\n",
    "   - how squared residual error is represented in the model\n",
    "   - Need to minimize MSE equation to best fit model\n",
    "     - Solution: Find best parameters (theta)\n",
    "\n",
    "#### How to estimate theta (parameter):\n",
    "1. Ordinary Least Squares:\n",
    "   - Tries to estimate the value of coefficients by minimizing mean square error\n",
    "   - Uses data as a matrix and applies linear algebra operations to estimate optimal values of theta\n",
    "   - **Downside:** It takes a very long time to perform matrix operations \n",
    "2. Optimization Approach:\n",
    "   - Iteratively minmizing error in the model\n",
    "   - Gradient Descent\n",
    "     - Starts optimization with random values for each coefficient \n",
    "     - Calculates error and tries to minimize error through changing value of coefficients in multiple iterations\n",
    "     - **Proper approach** for large datasets\n",
    "\n",
    "### Steps in prediction making:\n",
    "1. find parameters ($\\Theta$)\n",
    "2. Plug into the linear equation model ($y_\\theta = \\frac{1}{X_\\theta}$)\n",
    "- Eg. Co2 Emission = 125 + 6.2 Engine size + 14 Cylinders + ..... = 125 + 6.2 * 2.4 + 14 * 4 + ....  = 214.1\n",
    "\n",
    "### Caution:\n",
    "1. Adding too many independent variable will result in **overfitting** \n",
    "2. Should independent variables should be continuous\n",
    "   - Binary vairables (code categorical variables into numerical with dummy values eg: 0 for male and 1 for female)\n",
    "3. What are a linear relationships between dependent variable and independent variable ?\n",
    "   - use scatter plot to visually check the linearity \n",
    "   - If the relationship has no linearity use should use non-linear regression"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Non Linear Regression\n",
    "- If Scatter plot data shows a curvy line, linear regeression may not produce accurate predictions compared to non linear regression \n",
    "- There is a strong relationship between independent variable (GDP) and dependent variable (Year) but the relationship is non linear\n",
    "\n",
    "## What is non linear regression:\n",
    "- To model non-linear realationship between the dependent variable and a set of independent variables\n",
    "- yhat must be a non-linear function of the parameters theta, not necessarily the features x\n",
    "- It can be exponential, logistic etc. \n",
    "- Change of yhat depends on the changes in parameters theta\n",
    "\n",
    "### Note: \n",
    "- We Cannot use Ordinary Least Squares method to fit data \n",
    "- Estimation of parameters is not easy\n",
    "\n",
    "## estimation method: \n",
    "- Use exponential functions \n",
    "#### Polynomial Regressions\n",
    "- fits a curve line to data \n",
    "- Eg: Third degree polynomial equation ($\\hat{y} = \\theta_0 + \\theta_1x + \\theta_2 x_2 + \\theta_3 x_3$)\n",
    "- A polynomial regression model can be expressed/transformed into linear regression model\n",
    "  - Eg: $x_1 = x, x_2 = x^2, x_3 = x^3 $\n",
    "    - makes model into $\\hat{y} = \\theta_0 + \\theta_1 x_1 + \\theta_2 x_2 + \\theta_3 x_3$\n",
    "1. Linear Regression\n",
    "2. Quadratic (parabolic) Regression\n",
    "3. Cubic Regression\n",
    "4. etc.............\n",
    "\n",
    "## How to determine if the problem is linear or non-linear \n",
    "- Inspect visually \n",
    "  - Calcuate correlation coefficients between independent and dependent variables if the value is 0.7 or higher there is a linear tendency\n",
    "- Based on accurarcy\n",
    "   - When we cannot accurately model with linear parameters\n",
    "\n",
    "## How should I model my data, if it displays non-linear on a scatter plot ?\n",
    "- Polynomial regression\n",
    "- Non-linear regression model\n",
    "- Transform your data"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Classification\n",
    "- Supervised learning model\n",
    "- Categorizing some unknown items into a discrete set of categories and \"classes\"\n",
    "- The target attribute is a categorical variable\n",
    "\n",
    "- Classification determines label for a test case\n",
    "  - Eg: 1.  loan default classification (Which customers will have problem repaying loans)\n",
    "    - Build a classifier \n",
    "    - pass data to model\n",
    "    - classify data into defaultor or not a defaultor (Binary classifier)\n",
    "  - Eg: 2. To predict a category where a customer belong\n",
    "  - Eg: 3. Churn detection: To predict if a customer switches to another product/brand\n",
    "  - Eg: 4. Email / document classification\n",
    "\n",
    "## Types:\n",
    "- Binary classification\n",
    "- Multi-class classification \n",
    "\n",
    "## Algorithms\n",
    "1. Decision Trees (ID3, C4.5, C5.0)\n",
    "2. Naive Bayes\n",
    "3. Linear Discriminant Analysis\n",
    "4. k-Nearest Neighbor\n",
    "5. Logistic Regression\n",
    "6. Neural Networks\n",
    "7. Support Vector Machines (SVM)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# k-Nearest Neighbors (KNN)\n",
    "- Given the labelled data, we need to predict the label of unknown case\n",
    "\n",
    "## Build a classifier\n",
    "- If Age and Income are predictors of customercategory \n",
    "\n",
    "### Note:\n",
    "  1. Poor Judgement: can we find closest cases and assign value to new case (1st nearest neighbor)\n",
    "  2. Good Judgement: Finding highest number of times a class appears in the neighborhood\n",
    "\n",
    "### Definition:\n",
    "- A method for classifying cases based on their similarity to other cases\n",
    "- Cases nearer to each other are \"neighbors\"\n",
    "- Distance between two cases is the measure of \"dissimilarity\"/ \"similarity\"\n",
    "- Distance measured using Euclid distance\n",
    "\n",
    "### Usage:\n",
    "1. Pick a value for k\n",
    "   - How to choose\n",
    "     - low value of k would result in less accurate model (causes over fitting)\n",
    "     - High value of k would result in overly generalized model (causes under fitting)\n",
    "     - *Solution:* Reserve part of data to testing the accuracy of the model - pick k value with highest accuracy\n",
    "\n",
    "2. Calculate the distance of unknown case from all cases\n",
    "   - Need to normalize feature set to get accurate dissimilarity measure\n",
    "   - Highly depend on data type and domain knowledge\n",
    "3. Select the k-observations in the training data that are \"nearest\" to the unknown data point\n",
    "4. predict the response of the unknown data point using the most popular response value from the K-nearest neighbors"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Evaluation metrics for classification\n",
    "## Classification accuracy Methods:\n",
    "- Compare actual labels in test set vs predicted labels by model\n",
    "\n",
    "1. Jaccard Index / Jaccard similarity coefficients:\n",
    "- If model predicts 8 values accurately out of 10. Jaccard index will be $J(y,\\hat{y})$ = 8 / (10 + 10 - 8) = 0.66\n",
    "- Highest accuracy is 1.0 ; lowest accuracy is 0.0\n",
    "\n",
    "2. F1-score: (Confusion Matrix):\n",
    "\n",
    "| Confusion Matrix  | Predicted Label 1 | Predicted Label 0 |\n",
    "|-------------------| ------------------| ------------------|\n",
    "|  Actual Label 1   |        6          |        9          |\n",
    "|  Actual Label 0   |        1          |        24         |\n",
    "\n",
    "- Precision = $\\frac{TP} {(TP + FP)}$\n",
    "- Recall = $\\frac{TP}{(TP + FN)}$\n",
    "\n",
    "**$F1 Score (Harmonic progression of Precision & Recall)$ = $2 * \\frac {precision * recall} {precision + recall}$**\n",
    "- Highest accuracy is 1.0 (shows model have perfect precision and recall)\n",
    "\n",
    "3. Log loss:\n",
    "- Sometimes out of a classifier is a probability of a class label instead of the label between 0 and 1\n",
    "- Measure log loss value for each row using log loss equation $(y * log(\\hat{y}) + (1-y) * log(1-\\hat{y}))$\n",
    "- Calculate average log loss using $-\\frac{1}{n} \\Sigma(y * log(\\hat{y}) + (1-y) * log(1-\\hat{y}))$\n",
    "- Ideal classifiers have lower log loss (Classifier with lower log loss has better accuracy) "
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Decision Trees\n",
    "- What drug is needed for similar patient based on other patients data\n",
    "- Building decision tree with training set\n",
    "- Testing an attribute and branching the cases\n",
    "### Interpretation\n",
    "1. Each internal node corresponds to a test (Eg: Sex)\n",
    "2. Each branch corresponds to a result of the test (Eg: Male)\n",
    "3. Each leaf node assigns a patient to a class (Eg: Drug B)\n",
    "\n",
    "### Building a decision tree algorithm\n",
    "1. Choose an attribute from your dataset\n",
    "2. Calculate the significance of attribute in splitting of data\n",
    "   - Recursive partitioning\n",
    "      - Which attribute is needed for best prediction\n",
    "      - The selected attribute should have a defined outcome\n",
    "      - Results in the node are mostly pure (Pure Node means: 100 % of case node fall into a specific category)\n",
    "      - Key factors (More predictiveness, Less Impurity in nodes, Lower Entropy)\n",
    "      - If attribute is uncertain - split it into \"sub-tree\"\n",
    "    - Impurity of a node:\n",
    "      - Must be decreased by each sub-tree\n",
    "      - Calculated by entropy of the data\n",
    "    - Entropy:\n",
    "      - Amount of information disorder or amount of randomness in data\n",
    "      - used to calculate homogeneity of samples in that node\n",
    "        - Completely homogeneous entropy is 0\n",
    "        - Samples are equally divided it has entropy of 1\n",
    "        - 1 Drug A 7 Drug B - low entropy\n",
    "        - 3 Drug A 5 Drug B - High entropy\n",
    "        - 0 Drug A 8 Drug B - 0 entropy\n",
    "        - 4 Drug A 4 Drug B - 1 entropy\n",
    "      - Entropy measured with a formula.i.e.  $-p(A)log(p(A)) - p(B)log(p(B))$\n",
    "\n",
    " Note: While choosing which attribute to split use \"information gain\"\n",
    " - Information gain is the information that can increase level of certainty after splitting\n",
    " - Information gain = (Entropy before split) - (weighted entropy after split)\n",
    "   - $Weighted~entropy~after~split = \\frac {no.~of~females}{total~patients} * entropy~after~split + \\frac {no.~of~males} {total~patients} * entropy~after~split$\n",
    " - As weighted entropy decreases, Information gain increases\n",
    "\n",
    "3. Split data based on the value of the best attribute\n",
    "4. Repeat (Step 1) for next attribute"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Logistic Regression\n",
    "- Used in classification for categorical variables\n",
    "- one or more independent variables to predict dependent variable\n",
    "- Independent variables should be continuous (if categorical need to create dummy continuous values)\n",
    "- used in both binary class classification and multi class classification\n",
    "\n",
    "### Applications:\n",
    "1. Predict probability of heart attack based on BMI and age etc.\n",
    "2. Predict default on a mortgage\n",
    "\n",
    "### When to use:\n",
    "1. Target field is categorical / binary\n",
    "2. If you need probability results (eg: probability of buying a product)\n",
    "3. When you need a linear decision boundary\n",
    "4. If you need to understand the impact of a feature\n",
    "\n",
    "### Mathematical representation\n",
    "- $\\hat{y} = P(y = 1 | x)$\n",
    "- $P(y = 0 | x) = 1- P(y = 1 |x)$"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Logistic regression vs linear regression\n",
    "- In logistic regression  (predict categorical value)\n",
    "  - $\\hat{y}$ = P(y = 1 | x)\n",
    "  - $\\hat{y}$ is the predicted labels of our model\n",
    "  - y is the actual label in dataset\n",
    "\n",
    " $\\Theta^T$ X = $\\theta_0$ + $\\theta_1$ * $$x_1$$\n",
    "\n",
    "### Threshold: (Step function)\n",
    " - $\\hat{y}$ = 0 if $\\Theta^T$ X < 0.5\n",
    " - $\\hat{y}$ = 1 if $\\Theta^T$ X $\\ge$ 0.5\n",
    "\n",
    " ### Limitations:\n",
    " 1. Doesn't distinguish customer with 1 or 1000 in $\\Theta^T$ X value (X-axis)\n",
    " 2. Not accurate\n",
    "\n",
    " ### Sigmoid function:\n",
    " - Sigmoid / Logistic function is similar to step function but used in logistic regression\n",
    " - $\\Sigma(\\Theta^TX)$ = $$\\frac{1} {1 + e^-\\Theta^Tx }$$\n",
    " - When $\\Theta^T x$ gets big value of $\\Sigma(\\Theta^TX)$ will get closer to 1\n",
    " - When $\\Theta^T x$ gets small value of $\\Sigma(\\Theta^TX)$ will get closer to 0\n",
    "\n",
    " - When sigmoid value gets closer to 1 the probability of y given x (P(y = 1|x) goes up\n",
    " - When sigmoid value gets closer to 0 the probability of y given x (P(y = 1|x) goes down\n",
    "\n",
    " - Gives Point belonging to a class instead of value y directly\n",
    " - $\\Sigma(\\Theta^T X)$ = $\\Sigma(\\Theta_0 + \\Theta_1x_1 + ....)$\n",
    " - It always returns value between 0 and 1\n",
    "\n",
    " - New model is with sigmoid function is\n",
    " - $\\hat{y}$ = $\\Sigma(\\Theta^T X)$\n",
    "\n",
    "### Training process\n",
    "1. Initiate $\\Theta$ with random values\n",
    "   - Example pick $\\Theta$ = [-1,2]\n",
    "2. Calculate model output $\\hat{y}$ = $\\Sigma(\\Theta^T X)$ for a customer in training set\n",
    "   - X and $\\Theta^TX$ are the feature values for example age and income of the customer (for example = [2,5])\n",
    "3. Compare output value $\\hat{y}$ with actual value y and record the error\n",
    "4. Calculate error for all customers and add up this errors\n",
    "   - Total error is calculated by cost function; Cost = J($\\Theta$)\n",
    "   - Cost function shows how poorly model is estimating labels\n",
    "   - Lower the cost, better the model is at estimating labels correctly\n",
    "5. Minimize Cost function\n",
    "   - Change $\\Theta$ value to reduce the cost\n",
    "   - This creates $\\Theta_{new}$\n",
    "6. Go to Step 2 (another iteration) and continue till cost is low enough\n",
    "\n",
    "### How to identify $\\Theta_{new}$ and when to stop the iteration ?\n",
    "Most popular solution - Gradient Descent4"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Training logistic regression model\n",
    "\n",
    "- General cost function\n",
    "  - change the weight to reduce cost\n",
    "  - find the relationship between cost function and $\\theta$\n",
    "  - formula:\n",
    "    - $Cost(\\hat{y}$, y) =  $\\frac{1}{2}(\\Sigma(\\theta^TX)-y)^2$\n",
    "    - Interpretation of above equation\n",
    "      - captures Predicted value - actual value\n",
    "      - square value is used to remove possibility of negative results\n",
    "    - Final, Mean Squared Error\n",
    "      - J($\\theta$) = $\\frac{1}{m} \\Sigma_{i=1}^{m} Cost(\\hat{y}, y)$\n",
    " - Need to find Global minimum for this function\n",
    "\n",
    "### Plotting the cost function of the model\n",
    "\n",
    "- Actual value of y = 1 or 0\n",
    "- If y = 1 and $\\hat{y}$ = 1 then Cost = 0\n",
    "- If y = 1 and $\\hat{y}$ = 0 then cost = large\n",
    "\n",
    "- Cost($\\hat{y}$, y) = -log($\\hat{y}$) ;  if y = 1\n",
    "- Cost($\\hat{y}$, y) = -log(1-$\\hat{y}$) ; if y = 0\n",
    "\n",
    "Logistic Regression Cost function:\n",
    "\n",
    "$J(\\Theta)$ = -$\\frac{1}{m} \\Sigma_{i=1}^my^ilog(\\hat{y}^i) + (1-y^i)log(1-\\hat{y}^i)$\n",
    "\n",
    "### Gradient Descent:\n",
    "- Iterative approach to find minimum of J($\\theta$)\n",
    "- A technique to use the derivative of a cost function to change the parameter values, in order to minimize the cost\n",
    "- How it works ?\n",
    "  - Draw a contour chart with errors also called error bowl\n",
    "  - Aim is to find best parameters to minimize cost value\n",
    "  - Select a random point on the bowl\n",
    "  - As long as we are going downward we can go one more step (steeper the slope, we can take more steps downward)\n",
    "  - When we are approaching to minimum value slope diminishes, then we will take smaller steps till we reach a flat surface.\n",
    "- How would you measure how many steps to take ?\n",
    "  - By calculating gradient descent of the cost function at that point\n",
    "  - Gradient is the slope of the surface at every point\n",
    "  - Direction of the gradient is the direction of the greatest uphill\n",
    "- How do we calculate gradient of a cost function at a point ?\n",
    "  - If you select a random point on the error bowl and take partial derivative of J($\\theta$) with respect to each parameter at that point, we will find the slope of the move at that point.\n",
    "  - If we move opposite direction of the slope we will move opposite direction of the error curve\n",
    "  - For example if we measure $\\frac {\\partial{J}}{\\partial\\theta_1}$ we will find out it is a positive number\n",
    "    - This indicates that function is increase when $\\theta_1$ is increasing\n",
    "    - So, to decrease J, we need to move toward opposite direction (Which means we should decrease $\\theta_1$)\n",
    "  - How big step to take ?\n",
    "    - Gradient value indicates, how big step to take\n",
    "    - If slope is large, we should take a large step because we are far from the minimum\n",
    "  - Partial derivative of J is calculated using this expression\n",
    "    - $\\frac {\\partial J} {\\partial \\theta_1 }$ = -$\\frac {1}{m} \\Sigma_{i=1}^m(y^i-\\hat{y}^i)x_1^i$\n",
    "  - A vector of all the slopes calculated using partial derivative is called \"gradient vector\"\n",
    "    - gradient vector is used to update/change all the parameters\n",
    "    - Take previous value of parameters and substract the error derivative\n",
    "      - New$\\theta$ = old$\\theta$ - $\\mu\\nabla$J\n",
    "        - $\\mu$ is the learning rate\n",
    "\n",
    "### Overall steps:\n",
    "1. Initialize the parameters randomly\n",
    "2. Feed the cost function with training set, and calculate the error\n",
    "3. Calculate the gradient of the cost function\n",
    "4. Update weights with new values\n",
    "5. Go to step 2 until cost is small enough\n",
    "6. Predict the new customer X"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Support Vector Machines\n",
    "- SVM is a supervised algorithm that classifies cases by finding a separator\n",
    "1. Mapping data to a high dimensional feature space (In this step data point will be categorized)\n",
    "2. Finding a separator (A separator will be estimated for the data)\n",
    "\n",
    "### Data Transformation:\n",
    "1. Make data separable\n",
    "2. In one dimensional space all point are in a single line so it is inseparable so it needs to be converted to two dimensional space.\n",
    "   - Map data using a function $\\phi$(x) = [x, $x^2$]\n",
    "   - In 2D a hyperplane is a line dividing data into two parts\n",
    "3. Mapping data into higher dimensions is called \"kernelling\"\n",
    "   - Types of Kernelling\n",
    "     - Linear\n",
    "     - Polynomial\n",
    "     - RBF (Radial Basis Function)\n",
    "     - Sigmoid\n",
    "   - There is no best Kernelling algorithm, usually perform the Kernelling and compare the results to choose which fits the data\n",
    "\n",
    "### How to find hyperplane ?\n",
    "- In 2D hyperplane is a line that linearly separates the two class of data\n",
    "- Best way to find hyperplane:\n",
    "  - The line that creates largest separation or have highest margin between the two classes\n",
    "  -  Goal is to choose the hyperplane with highest margin\n",
    "- Examples closest to the margin are called \"Support Vectors\"\n",
    "  - Only support vectors examples matter for achieving our goal\n",
    "- Find a hyperplane in a way that it has the maximum distance from Support Vectors\n",
    "  - Hyper planes and boundary decision lines have their own equation\n",
    "    - Decision Boundary $w^T$ x + b = 1 and $w^T$ x + b = -1\n",
    "    - Hyperplane $w^T$ x + b = 0\n",
    "    - Find the value of w and b such that $\\phi$(w) = $\\frac{1}{2} w^Tw$ is minimized;\n",
    "    - for all {($x_i, y_i)$}: $y_i(w^Tx_i + b) \\ge 1$\n",
    "- Finding correct margin is an optimization problem\n",
    "  - Like all optimization problems, this can be solved using gradient descent\n",
    "\n",
    "### Pros and Cons of SVM:\n",
    "#### Pros:\n",
    "1. Accurate in high dimensional spaces\n",
    "2. Uses a subset of training data so memory efficient\n",
    "#### Cons:\n",
    "1. Prone for Over fitting\n",
    "2. SVM does not provide probability estimates directly which are needed in most classification problems\n",
    "3. They are not efficient if your data is larger than 1000 rows\n",
    "\n",
    "### SVM Applications:\n",
    "1. Good for Image Classification\n",
    "2. Effective in text mining tasks (Detecting spam, sentiment analysis)\n",
    "3. Gene expression classification\n",
    "4. Regression, outlier detection and clustering"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Clustering\n",
    "- Unsupervised\n",
    "- Definition: A group of objects that are similar to other objects in the cluster, and dissimilar to data points in other clusters.\n",
    "## Introduction\n",
    "- To apply customer segmentation on historical data\n",
    "- To identify similar customers\n",
    "- Partitions customers into groups that are mutually exclusive\n",
    "\n",
    "## Clustering vs classification\n",
    "- Classification should have a labeled data set as training data\n",
    "  - Predicts data using\n",
    "    - decision tree\n",
    "    - logistic regression\n",
    "    - SVM\n",
    "- Clustering works with unlabeled dataset\n",
    "  - Group similar customers\n",
    "    - K means\n",
    "## Application of Clustering applications\n",
    "- Retail/ marketing\n",
    "  - Identifying buying pattern of customers\n",
    "  - Recommending new books or movies to new customers\n",
    "- Banking\n",
    "  - Fraud detection in credit card use\n",
    "  - Identifying clusters of customers (Eg. loyal)\n",
    "- Insurance\n",
    "  - Fraud detection and claims analysis\n",
    "- Publication\n",
    "  - Auto-categorizing news based on their content\n",
    "  - Recommending similar news articles\n",
    "- medicine\n",
    "  - Characterize patient behaviour\n",
    "- Biology\n",
    "  - Group genes with similar expression patterns and similar markers\n",
    "\n",
    "## Why Clustering?\n",
    "- Exploratory data analysis\n",
    "- Summary generation\n",
    "- Outlier detection\n",
    "- Finding duplicates\n",
    "- Pre-processing step\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# k-Means Clustering\n",
    "- partitioning clustering\n",
    "- partition customer base into similar groups\n",
    "- k-means divides the data into non-overlapping subsets (clusters) without any cluster internal structure\n",
    "  - Un supervised learning\n",
    "\n",
    "## Determine similarity or dissimilarity\n",
    "- k means\n",
    "  - Minimizes Intra-cluster distances\n",
    "  - Maximizes Inter-cluster distances\n",
    "\n",
    "## How to calculate dis similarity beteen two cases\n",
    "- 1D (dimensional) similarity / distance (where only one feature is present eg: Age)\n",
    "- 2D (dimensional) space ... Multi-dimensional space\n",
    "- Minkowski distance: (Euclidean distance)\n",
    "  - $ Dis (x1,x2) = \\sqrt{\\sigma_{i=0}^n (x_{1i} - x_{2i})^2} $\n",
    "- To use Data needs to be normalized first\n",
    "### Possible approaches:\n",
    "1. Euclidean distance\n",
    "2. cosine similarity\n",
    "3. Average distance\n",
    "\n",
    "## How does k-means work ?\n",
    "- For Example: Take a 2D customers segmentation based age and height\n",
    "### Steps:\n",
    "- 1. Initialize k = 3\n",
    "     - How to choose ?\n",
    "       - pick thre values from centroid from dataset\n",
    "       - Cerate three random points\n",
    "     - Should be of same feature size as customer feature set\n",
    "- 2. Calculate distance from each centroid point\n",
    "     - Each row represents distance from each centroid (This is called distance matrix)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}